Model already exists: ../sfd-main/src/cifar10/edm-cifar10-32x32-uncond-vp.pkl
Loading the pre-trained diffusion model from "../sfd-main/src/cifar10/edm-cifar10-32x32-uncond-vp.pkl"...
cherish
4
torch.Size([12])
teacher now
torch.Size([3])
/home/zeus/miniconda3/envs/cloudspace/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
Total parameters in U-Net:      60417164
Setting up optimizer...
Training for 200 kimg...

diff
tensor(4.6492e-06, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(4.4703e-06, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(4.6492e-06, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(0.0031, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0031, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0031, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0031, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
=========
Step: 0 | Loss-mean: 1131.16162109 | loss-std:  15.64160156
diff
tensor(0.0084, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(3.3200e-05, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(4.6492e-06, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(0.0223, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0184, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0184, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0184, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
=========
Step: 1 | Loss-mean: 112.92321014 | loss-std:   1.64582932
diff
tensor(0.0140, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0001, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(2.2292e-05, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(0.0312, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0263, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0263, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0263, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
=========
Step: 2 | Loss-mean:  27.14051437 | loss-std:   6.25552940
tick 0     kimg 0.3       time 35s          sec/tick 32.2    sec/kimg 125.78  maintenance 3.2    gpumem 20.11  reserved 20.26 
diff
tensor(0.0186, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0002, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(3.2067e-05, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(0.0137, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0086, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0086, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0086, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
=========
Step: 0 | Loss-mean: 1131.31750488 | loss-std:  14.11830235
diff
tensor(0.0233, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0002, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(4.9114e-05, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(0.0331, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0105, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0104, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0104, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
=========
Step: 1 | Loss-mean: 112.77191162 | loss-std:   1.52794766
diff
tensor(0.0205, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0002, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(5.0306e-05, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(0.0502, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0274, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0273, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0273, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
=========
Step: 2 | Loss-mean:  27.53413391 | loss-std:   7.28294468
diff
tensor(0.0330, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0004, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(8.7380e-05, device='cuda:0', dtype=torch.float16,
       grad_fn=<MeanBackward0>)
tensor(0.0342, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0201, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0201, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0201, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
=========
Step: 0 | Loss-mean: 1132.22106934 | loss-std:  13.01460743
diff
tensor(0.0362, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0005, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0001, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0533, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0175, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0172, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0172, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
=========
Step: 1 | Loss-mean: 112.57910156 | loss-std:   1.38198030
diff
tensor(0.0276, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0006, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0001, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)
tensor(0.0850, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0441, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0435, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
tensor(0.0435, device='cuda:0', dtype=torch.float16, grad_fn=<StdBackward0>)
=========
Step: 2 | Loss-mean:  26.39438629 | loss-std:   6.37933016

Aborted!
